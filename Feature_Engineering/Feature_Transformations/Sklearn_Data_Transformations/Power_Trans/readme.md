### **Power Transformer**  
A **Power Transformer** is used to make data more **normally distributed** by reducing **skewness** and **stabilizing variance**. It is useful for models like **linear regression, k-means, and PCA**, which assume normally distributed data.  

#### **Types of Power Transformation:**  
1. **Box-Cox Transformation** â€“ Works **only for positive values**.  
2. **Yeo-Johnson Transformation** â€“ Works for **both positive and negative values**.  

---

### **Box-Cox Transformation**  
Transforms **right-skewed data** to make it more symmetric.  

#### **Formula:**  

- If `Î» â‰  0`: `y = (x^Î» - 1) / Î»`
- If `Î» = 0`: `y = log(x)` 

âœ… **Only for positive values**  
âœ… **Stabilizes variance and normalizes distribution**  

---

### **Yeo-Johnson Transformation**  
Extends Box-Cox to handle **zero and negative values**.  

#### **Formula:**  
- If `x â‰¥ 0`:
  - `y = ((x + 1)^Î» - 1) / Î»`, if `Î» â‰  0`
  - `y = log(x + 1)`, if `Î» = 0`
- If `x < 0`:
  - `y = ((1 - x)^(2 - Î») - 1) / (2 - Î»)`, if `Î» â‰  2`
  - `y = -log(1 - x)`, if `Î» = 2` 

âœ… **Works for both positive and negative values**  
âœ… **Handles zero values effectively**  

---

### **Comparison**  

| Transformation  | Works for | Best Used For |
|---------------|-----------|---------------|
| **Box-Cox**   | Positive values only | Right-skewed data |
| **Yeo-Johnson** | Positive & negative values | Data with zeros/negatives |

Both are implemented in **scikit-learnâ€™s** `PowerTransformer` for preprocessing data before applying machine learning models. ðŸš€
